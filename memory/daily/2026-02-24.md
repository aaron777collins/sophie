# Daily Log - 2026-02-24

## Pipeline Critical Fix - [08:15 EST]

### Issue Identified
Aaron reported that pipeline.log files contained "No log files found" instead of actual verbose logs.

**Root Cause:** Logger class uses dependency injection via StandardDependencyInjection. First pipeline's PathProvider gets cached and reused by ALL subsequent pipelines. This caused all log files to be written to `logs/basic_100km_const/` regardless of which pipeline was running.

**Evidence:**
- `logs/basic_100km_const/` contained: `basic_100km_rand.txt`, `basic_200km_const.txt`, etc.
- Other pipeline directories like `logs/basic_100km_rand/` never existed
- `consolidate_logs()` looked for `logs/{pipeline_name}/` which didn't exist for most pipelines

### Fix Applied - [08:20 EST]
Modified `run_all_pipelines.py` to capture stdout/stderr directly using TeeWriter class:
- Bypasses broken Logger dependency injection
- Captures verbose output including row counts, vehicle IDs, ML metrics
- Runs with PYTHONUNBUFFERED=1 for real-time logging

### Actions Taken
1. Backed up original: `run_all_pipelines.py.backup`
2. Deployed fix via SCP
3. Cleared all artifacts:
   - `/var/www/static/pipeline-results/*`
   - `data/classifierdata/splitfiles/combinedcleaned/*`
   - `data/mclassifierdata/*`
   - `Outputs/*`
   - `logs/*`
4. Started fresh 36-pipeline run

### Pipeline Progress (basic_100km_const)
- Total rows: 3,435,803
- Vehicle IDs: 1,662 total
- Train: 2,748,642 rows, 1,336 vehicles, 401 attackers
- Test: 687,161 rows, 353 vehicles, 106 attackers

### Notifications Sent - [08:30 EST]
- Email to aaron777collins@gmail.com & joshuapicchioni@gmail.com
- Coordinator inbox updated

### Monitoring
- Dashboard: http://65.108.237.46/pipeline-results/
- Log: `ssh jaekel 'tail -f /tmp/run_all_fresh.log'`

## [12:18 EST] Pipeline Restart with Skip Logic

**Issue:** `swap_rand` attack on 100km data (3.4M rows) caused Dask deadlock. Process was killed.

**Decision:** Aaron approved skipping heavy attacks for large datasets.

**Skipped combinations (100km and 200km):**
- `swap_rand` — row-by-row swapping deadlocks on large datasets
- `override_const` — same memory issue
- `override_rand` — same memory issue

**Impact:**
- Original: 162 pipelines (9 features × 3 radii × 6 attacks)
- New: 108 pipelines (skipping 54 heavy combos)
- 2km still gets all 6 attacks
- 100km/200km get 3 attacks each (offset variants only)

**Reason documented in code:** `run_with_skips.py` wrapper script added with comments explaining the Dask deadlock on 3.4M+ rows.

**Progress before restart:**
- 9/162 completed (all 2km basic + 3 offset attacks for 100km basic)

**Remaining:** 99 pipelines
**ETA:** ~6-8 hours

---

## [04:00 EST] Wyoming Pipeline Investigation Complete

**Issue:** Aaron noticed spatial filter was returning same ~2,000 rows regardless of radius
**Root cause:** The GeneratorPathProvider singleton was being reused, causing different radii to share cache paths
**Also found:** `num_subsection_rows=100000` was limiting to 100K sample instead of full 13.3M dataset

**Fixes Applied:**
1. Clear `SingletonABCMeta._instances` before each pipeline run
2. Set `num_subsection_rows=None` to use full dataset

**Results:**
- 2km radius: 238,738 rows (vs 1,954 before - 122x increase!)
- Caching now correctly separates different configurations
- Full 162-pipeline run started, estimated 10-14 hours

**Opus sub-agent** also investigated and reached same conclusion independently.

**Status:** Full pipeline run in progress, will complete overnight.
