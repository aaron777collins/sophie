# Daily Log — 2026-02-05

## Dask Pipeline Debugging Marathon (00:00-00:45 EST)

Major debugging session on Jaekel server fixing multiple Dask pipeline issues. Aaron checked in around 23:30 asking if still chugging along.

### Bug Fix 1: Wrong Cleaner Class (~00:00 EST)
- **Problem:** All 11 DaskMClassifier*.py files imported `CleanWithTimestamps` (pandas version) instead of `DaskCleanWithTimestamps` (Dask version)
- **Symptom:** `isinstance(data, pd.DataFrame)` failed on Dask DataFrames → ValueError
- **Fix:** Updated all 11 files: import + cleanerClass + cleanFunc references → `DaskCleanWithTimestamps`
- **Commit:** `7e6f327` — "fix: Use DaskCleanWithTimestamps instead of pandas CleanWithTimestamps"

### Bug Fix 2: head(npartitions=1) Gets Wrong Time Period (~00:30 EST)
- **Problem:** `DaskDataGatherer.head(npartitions=1)` reads only from first partition → July 2019 data
- **But pipeline filters for:** April 2021 + within 2000m of origin
- **Result:** 0 rows after filtering → `ValueError: n_samples=0`
- **Attempted fix:** Changed to `sample(frac=N/total)` across all partitions
- **Commit:** `584bbe7` — "fix: Use sample() instead of head() for representative subsection"
- **Outcome:** Still 0 rows — even with representative sampling, 100K rows from 45.5M is too small (only ~4,642 rows survive the spatial-temporal filter in the full dataset = 0.01%)

### Bug Fix 3: numSubsectionRows Too Small (~00:40 EST)
- **Problem:** `numSubsectionRows = 100000` → way too few rows survive spatial-temporal filter
- **Root cause discovered:** The working `wyoming40gb` pipeline used `numSubsectionRows = -1` (ALL rows)
- **Evidence:** First partition = July 2019 data; April 2021 data exists only in later partitions
- **Fix:** Changed all 11 DaskMClassifier*.py: `numSubsectionRows = -1`
- **Commit:** `ab69da4` — "fix: Use full dataset (numSubsectionRows=-1) instead of 100K subsection"

### Current Status (00:45 EST)
- Test pipeline `DaskMClassifierRandOffset10To20.py` running with full 45.5M rows
- Processing the cleaning/filtering step (heavy XY coordinate conversion)
- Using cached `subsection-1.parquet` (91 partitions from earlier wyoming40gb run)
- Memory usage: 12GB/62GB — well within limits
- **Waiting to see if this produces non-zero rows after filtering**

### Key Technical Insights
- Wyoming CV data spans July 2019 → late 2021 across 91 partitions (500K rows each)
- Spatial-temporal filter (April 2021 + 2000m radius around -106°/41°) is extremely selective: ~4,642 rows from 45.5M (0.01%)
- Must process ALL rows to get enough data for ML training
- Earlier successful pipeline took ~20 minutes for full dataset processing

### Pipeline Queue Daemon
- Location: `/home/ubuntu/.pipeline-queue/daemon.py` on Jaekel
- Batch ID for current test: `20260205_053543`
- Results: `/var/www/static/pipeline-results/{batch_id}/{pipeline_name}/`
- Dashboard: `http://65.108.237.46:5000/`

### All Commits Today (Jaekel → GitHub)
1. `7e6f327` — DaskCleanWithTimestamps fix
2. `584bbe7` — sample() fix (intermediate, may not be needed with -1)
3. `ab69da4` — numSubsectionRows = -1

### Caches Cleared
- `data/classifierdata/subsection/*` (except subsection-1.parquet reused)
- `data/classifierdata/splitfiles/*`
- `data/classifierdata/combinedcleaned/*`
- `data/mclassifierdata/cleaned/*`
- `Outputs/Output/`
