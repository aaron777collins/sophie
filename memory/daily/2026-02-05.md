# Daily Log â€” 2026-02-05

## Dask Pipeline Debugging Marathon (00:00-00:45 EST)

Major debugging session on Jaekel server fixing multiple Dask pipeline issues. Aaron checked in around 23:30 asking if still chugging along.

### Bug Fix 1: Wrong Cleaner Class (~00:00 EST)
- **Problem:** All 11 DaskMClassifier*.py files imported `CleanWithTimestamps` (pandas version) instead of `DaskCleanWithTimestamps` (Dask version)
- **Symptom:** `isinstance(data, pd.DataFrame)` failed on Dask DataFrames â†’ ValueError
- **Fix:** Updated all 11 files: import + cleanerClass + cleanFunc references â†’ `DaskCleanWithTimestamps`
- **Commit:** `7e6f327` â€” "fix: Use DaskCleanWithTimestamps instead of pandas CleanWithTimestamps"

### Bug Fix 2: head(npartitions=1) Gets Wrong Time Period (~00:30 EST)
- **Problem:** `DaskDataGatherer.head(npartitions=1)` reads only from first partition â†’ July 2019 data
- **But pipeline filters for:** April 2021 + within 2000m of origin
- **Result:** 0 rows after filtering â†’ `ValueError: n_samples=0`
- **Attempted fix:** Changed to `sample(frac=N/total)` across all partitions
- **Commit:** `584bbe7` â€” "fix: Use sample() instead of head() for representative subsection"
- **Outcome:** Still 0 rows â€” even with representative sampling, 100K rows from 45.5M is too small (only ~4,642 rows survive the spatial-temporal filter in the full dataset = 0.01%)

### Bug Fix 3: numSubsectionRows Too Small (~00:40 EST)
- **Problem:** `numSubsectionRows = 100000` â†’ way too few rows survive spatial-temporal filter
- **Root cause discovered:** The working `wyoming40gb` pipeline used `numSubsectionRows = -1` (ALL rows)
- **Evidence:** First partition = July 2019 data; April 2021 data exists only in later partitions
- **Fix:** Changed all 11 DaskMClassifier*.py: `numSubsectionRows = -1`
- **Commit:** `ab69da4` â€” "fix: Use full dataset (numSubsectionRows=-1) instead of 100K subsection"

### Bug Fix 4: Wrong Attacker Classes (~00:00 EST)
- **Problem:** `add_attacks_positional_offset_rand()` returned None
- **Root cause:** `StandardPositionFromOriginAttacker` doesn't have offset methods (only override methods). The base interface `IConnectedDrivingAttacker` has a stub that returns `None`.
- **Fix:** 
  - RandOffset pipelines â†’ `StandardPositionalOffsetAttacker`
  - RandomPos â†’ use `add_attacks_positional_override_rand` (correct method)
  - PositionSwap â†’ `StandardPositionalOffsetAttacker` (has swap method)
  - Train/test split â†’ 80/20 proportional instead of hardcoded 100K
- **Commit:** `3472a89` â€” "fix: Correct attacker classes + proportional train/test split"

### SUCCESS! (01:05 EST)
**Pipeline completed successfully!** Runtime: 4m 39s

**Results (RandOffset 10-20m attack detection):**
| Classifier | Test Accuracy | Test F1 | Test Precision |
|------------|---------------|---------|----------------|
| RandomForest | 63.54% | 0.58 | 83.71% |
| KNeighbors | 55.65% | 0.48 | 88.07% |
| DecisionTree | 51.35% | 0.47 | 72.66% |

- Confusion matrices saved to `data/mclassifierdata/results/DaskMClassifierRandOffset10To20/`
- CSV aggregation file not written (minor config issue, results still valid)

### Key Technical Insights
- Wyoming CV data spans July 2019 â†’ late 2021 across 91 partitions (500K rows each)
- Spatial-temporal filter (April 2021 + 2000m radius around -106Â°/41Â°) is extremely selective: ~4,642 rows from 45.5M (0.01%)
- Must process ALL rows to get enough data for ML training
- Earlier successful pipeline took ~20 minutes for full dataset processing

### Pipeline Queue Daemon
- Location: `/home/ubuntu/.pipeline-queue/daemon.py` on Jaekel
- Batch ID for current test: `20260205_053543`
- Results: `/var/www/static/pipeline-results/{batch_id}/{pipeline_name}/`
- Dashboard: `http://65.108.237.46:5000/`

### All Commits Today (Jaekel â†’ GitHub)
1. `7e6f327` â€” DaskCleanWithTimestamps fix
2. `584bbe7` â€” sample() fix (intermediate, may not be needed with -1)
3. `ab69da4` â€” numSubsectionRows = -1

### Caches Cleared
- `data/classifierdata/subsection/*` (except subsection-1.parquet reused)
- `data/classifierdata/splitfiles/*`
- `data/classifierdata/combinedcleaned/*`
- `data/mclassifierdata/cleaned/*`
- `Outputs/Output/`

### Full Batch Run Started
- [21:33] First full pipeline (RandOffset10To20) completed: 4m 39s, all 3 classifiers trained
- [21:37] Added confusion matrix modal to dashboard (ðŸ“Š button in results table)
- [21:38] Queued all 10 remaining Dask pipelines:
  1. ConstOffsetPerID 100-200 (running)
  2. ConstOffsetPerID 50-100
  3. ConstOffsetPerIDWithRandDir 50-100
  4. ConstPosPerCar 100-200
  5. ConstPosPerCar 10-20
  6. ConstPosPerCar 50-100
  7. PositionSwap
  8. RandOffset 100-200
  9. RandOffset 50-100
  10. RandomPos 0-2000
- Estimated total time: ~50 minutes (5 min each)
